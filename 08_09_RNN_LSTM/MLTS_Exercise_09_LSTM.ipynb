{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MLTS Exercise 09 - LSTM Training</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to train a LSTM network on the electic power consumption dataset. The goal is to forcast values for the next day based on the mesurement values from a fixed window over the data from previous days.\n",
    "\n",
    "The dataset can be downloaded from [Individual Household Electric Power Consumption](https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption)\n",
    "\n",
    "It contains \"Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Reference**  \n",
    "Hebrail, G. & Berard, A. (2006). Individual Household Electric Power Consumption [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C58K54."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "path = 'data/household_power_consumption.txt'\n",
    "\n",
    "Household_consumption = pd.read_csv(path, sep=';', low_memory=False)\n",
    "# Household_consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data for training\n",
    "\n",
    "For our model, we just use the `Global_active_power` as a timeseries which we want to forcast. Therefore, we drop the other columns beforehand.\n",
    "\n",
    "Tasks:\n",
    "* Drop all columns except `Date`, `Time` and `Global_active_power`\n",
    "* Convert seperate date and time columns into datetime column\n",
    "* Convert numeric columns to correct type\n",
    "* Find and replace missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "Household_consumption.drop(columns=[\n",
    "    'Global_reactive_power', 'Voltage',\n",
    "    'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing date and time into a single datetime column\n",
    "Household_consumption['Datetime'] = pd.to_datetime(\n",
    "    Household_consumption['Date'] + ' ' + Household_consumption['Time'], \n",
    "    format='%d/%m/%Y %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "# Drop date and time column\n",
    "Household_consumption.drop(columns=['Date', 'Time'], inplace=True)\n",
    "# Drop rows with missing datetime\n",
    "Household_consumption.dropna(subset=['Datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numerical columns to numeric type\n",
    "numeric_column = 'Global_active_power'\n",
    "Household_consumption[numeric_column] = pd.to_numeric(Household_consumption[numeric_column], errors='coerce')\n",
    "\n",
    "# Household_consumption.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values\n",
    "missing_values = Household_consumption.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# Fill missing values with median for simplicity\n",
    "Household_consumption[numeric_column] = Household_consumption[numeric_column].fillna(\n",
    "    Household_consumption[numeric_column].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the datasets for training\n",
    "\n",
    "We want to forcased the mean value of the next day given the previous mean values. Therefore we need to resample our data to only contain one value per day.  \n",
    "\n",
    "This will make training fast and it should therefore also work on your laptops CPU. Additionally, you can train the model on Google Colab or on in the CIP pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to dayly intervals, calculating the mean for each day\n",
    "\n",
    "Household_consumption_daily = None  # TODO\n",
    "\n",
    "Household_consumption_daily.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data_week = Household_consumption_daily['Global_active_power']\n",
    "\n",
    "# Plot daily trends and rolling average\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_data_week.index, daily_data_week, label=\"Daily Global Active Power\", color='blue', alpha=0.6)\n",
    "plt.title(\"Daily Global Active Power Consumption\", fontsize=16)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Energy Consumption (kW)\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data between 0 and 1\n",
    "\n",
    "Normalizing or scaling the data is an important step before using the data for training as high raw values can often cause exploding gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Time Series to Supervised Format\n",
    "\n",
    "Time series data is sequential, but LSTMs require input-output pairs to learn patterns. By converting it into supervised format, we prepare the data so the model can learn from past observations to predict the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Time Series to Supervised Learning Format\n",
    "def create_supervised_data(data: np.ndarray, lag: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert time series data into supervised learning format.\n",
    "    `lag` determines how many previous time steps are used.\n",
    "    \"\"\"\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column (e.g., 'Global_active_power')\n",
    "target_column = None  # TODO\n",
    "\n",
    "# Use a lag of 30 (e.g., previous 30 time steps to predict the next step)\n",
    "lag = 30\n",
    "X, y = create_supervised_data(target_column, lag)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devide data into train and test set\n",
    "\n",
    "We do not split train and test set randomly but by year. This will give us a better estimate at how the model could perform over a longer period of time that has not been seen in the trainings set.\n",
    "\n",
    "For the test set, we will split away the last year of data, between '2010-01-01' and '2010-11-26'.  \n",
    "Therefore, the train set will be all the data between '2006-12-16' and '2010-01-01'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff for the test set (e.g., one year)\n",
    "test_start_date = '2010-01-01'  # Start of the test year\n",
    "test_end_date = '2010-11-26'    # End of the test year\n",
    "\n",
    "# TODO\n",
    "\n",
    "# Print shapes for confirmation\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LSTM Model\n",
    "\n",
    "Create your LSTM model here. You can either code it completely from scratch or use the already implemented models by pytorch.  \n",
    "\n",
    "Nevertheless, think about how your inputs and outputs will look like and how the data is processed through the dataset. How do you need to implement the LSTM model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LSTM model, the optimizer and the loss function. \n",
    "\n",
    "Loss function: MSELoss  \n",
    "Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the dataloaders for the model\n",
    "\n",
    "Convert the train and test set to torch, create a TorchDataset and a DataLoader for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing Loop\n",
    "\n",
    "Setup the training and test loop. Therefore, go through N epochs and in each epoch go through the data of your dataloader, pass the data to the model, calculate the loss and optimize the network. After each epoch, test the model on the test set by passing the data through the model and computing the loss. Save both test and train loss for later inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = None  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate your trained model by comparing the ground truth data against the predicted time series values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth vs. Predicted Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long term forcasting\n",
    "\n",
    "Additionally, we want to test if our model can do long term forcasting and predict correct values based on its previous predictions. Of course, our model is not designed and trained to do that specifically, but it is a good test how errors propagate over time and maybe gives us more ideas on how we could improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "* How well can the time series be predicted?\n",
    "* How could the training be improved / changed?\n",
    "* Can the data be modified to recieve better results?\n",
    "\n",
    "Feel free to test out more ideas as you like!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
